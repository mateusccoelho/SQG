{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import random\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from learning.treelstm.utils import load_word_vectors\n",
    "from learning.treelstm.trainer import Trainer\n",
    "from learning.treelstm.metrics import Metrics\n",
    "from learning.treelstm.model import *\n",
    "from learning.treelstm.vocab import Vocab\n",
    "import learning.treelstm.Constants as Constants\n",
    "from learning.treelstm.dataset import QGDataset\n",
    "from learning.treelstm.scripts.preprocess_lcquad import build_vocab\n",
    "from itertools import product\n",
    "\n",
    "data_path = '../learning/treelstm/data/lc_quad/'\n",
    "train_path = data_path + 'train/'\n",
    "dev_path = data_path + 'dev/'\n",
    "test_path = data_path + 'test/'\n",
    "checkpoints_path = '../learning/treelstm/checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Struct: pass\n",
    "args = Struct()\n",
    "args.seed = 41\n",
    "args.cuda = False\n",
    "args.batchsize = 20\n",
    "args.mem_dim = 150\n",
    "args.hidden_dim = 50\n",
    "args.num_classes = 2\n",
    "args.input_dim = 300\n",
    "args.sparse = False\n",
    "args.lr = 0.01\n",
    "args.wd = 1e-4\n",
    "\n",
    "args.epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.get_num_threads()\n",
    "torch.set_num_threads(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping words to indexes\n",
    "vocab = Vocab(\n",
    "    os.path.join(data_path, 'dataset.vocab'),\n",
    "    [Constants.PAD_WORD, Constants.UNK_WORD, Constants.BOS_WORD, Constants.EOS_WORD]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8057"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.idxToLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    emb = torch.load('glove_lc_merged_emb.pth')\n",
    "except:\n",
    "    emb = torch.Tensor(vocab.size(), 300).normal_(-0.05, 0.05)\n",
    "    # zero out the embeddings for padding and other special words if they are absent in vocab\n",
    "    for idx, item in enumerate([Constants.PAD_WORD, Constants.UNK_WORD, Constants.BOS_WORD, Constants.EOS_WORD]):\n",
    "        emb[idx].zero_()\n",
    "\n",
    "    with open('../learning/treelstm/data/glove.840B.300d.txt', 'r') as file:\n",
    "        for line in tqdm(file):\n",
    "            contents = line.rstrip('\\n').split(' ')\n",
    "            idx = vocab.getIndex(contents[0])\n",
    "            if(idx):\n",
    "                emb[idx] = torch.Tensor(list(map(float, contents[1:])))\n",
    "\n",
    "    torch.save(emb, 'glove_lc_merged_emb.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7896/7896 [00:00<00:00, 55352.86it/s]\n",
      "100%|██████████| 7896/7896 [00:00<00:00, 83627.02it/s]\n",
      "100%|██████████| 7896/7896 [00:00<00:00, 16924.59it/s]\n",
      "100%|██████████| 7896/7896 [00:00<00:00, 30231.59it/s]\n",
      "100%|██████████| 2265/2265 [00:00<00:00, 47000.18it/s]\n",
      "100%|██████████| 2265/2265 [00:00<00:00, 59091.24it/s]\n",
      "100%|██████████| 2265/2265 [00:00<00:00, 10590.26it/s]\n",
      "100%|██████████| 2265/2265 [00:00<00:00, 58802.66it/s]\n",
      "100%|██████████| 1090/1090 [00:00<00:00, 38241.35it/s]\n",
      "100%|██████████| 1090/1090 [00:00<00:00, 52868.36it/s]\n",
      "100%|██████████| 1090/1090 [00:00<00:00, 17461.38it/s]\n",
      "100%|██████████| 1090/1090 [00:00<00:00, 48279.12it/s]\n"
     ]
    }
   ],
   "source": [
    "train_set = QGDataset(train_path, vocab, args.num_classes)\n",
    "dev_set = QGDataset(dev_path, vocab, args.num_classes)\n",
    "test_set = QGDataset(test_path, vocab, args.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11251"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7896+2265+1090"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = DASimilarity(args.mem_dim, args.hidden_dim, args.num_classes)\n",
    "#similarity = CosSimilarity(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left - a - sent\n",
    "# right - b - query\n",
    "\n",
    "class SimilarityEncoders(nn.Module):\n",
    "    def __init__(self, vocab_size, in_dim, mem_dim, similarity, sparsity):\n",
    "        super(SimilarityEncoders, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, in_dim, padding_idx=Constants.PAD, sparse=sparsity)\n",
    "        self.sent_treelstm = ChildSumTreeLSTM(in_dim, mem_dim)\n",
    "        self.query_treelstm = ChildSumTreeLSTM(in_dim, mem_dim)\n",
    "        self.similarity = similarity\n",
    "\n",
    "    def forward(self, ltree, linputs, rtree, rinputs):\n",
    "        linputs = self.emb(linputs)\n",
    "        rinputs = self.emb(rinputs)\n",
    "        lstate, lhidden = self.sent_treelstm(ltree, linputs)\n",
    "        rstate, rhidden = self.query_treelstm(rtree, rinputs)\n",
    "        output = self.similarity(lstate, rstate)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimilarityEncoders(vocab.size(), args.input_dim, args.mem_dim, similarity, args.sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.KLDivLoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=args.lr, weight_decay=args.wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4c62a97a6fc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'emb' is not defined"
     ]
    }
   ],
   "source": [
    "# model.emb.weight.data.copy_(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(args, model, criterion, optimizer)\n",
    "metrics = Metrics(args.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7fa3a4580f68>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss, train_pred = trainer.test(train_set)\n",
    "# print('train_loss:', train_loss)\n",
    "# print('train_pred:', train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   0%|          | 0/7896 [00:00<?, ?it/s]/home/mateus/TCC/envSQG/lib/python3.5/site-packages/torch/nn/functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/mateus/TCC/envSQG/lib/python3.5/site-packages/torch/nn/functional.py:1339: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "../learning/treelstm/model.py:66: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = F.log_softmax(self.wp(out))\n",
      "/home/mateus/TCC/envSQG/lib/python3.5/site-packages/torch/nn/functional.py:1932: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "Training epoch 1: 100%|██████████| 7896/7896 [05:51<00:00, 22.44it/s]\n",
      "Testing epoch  1:   0%|          | 0/7896 [00:00<?, ?it/s]../learning/treelstm/trainer.py:50: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  linput, rinput = Var(lsent, volatile=True), Var(rsent, volatile=True)\n",
      "../learning/treelstm/trainer.py:51: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  target = Var(map_label_to_target(label, dataset.num_classes), volatile=True)\n",
      "Testing epoch  1: 100%|██████████| 7896/7896 [01:37<00:00, 80.90it/s] \n",
      "Training epoch 2:   0%|          | 3/7896 [00:00<05:38, 23.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: tensor(0.1514)\n",
      "train_pred: tensor([1.9249, 1.1357, 1.9325,  ..., 1.9290, 1.1791, 1.9251])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 2: 100%|██████████| 7896/7896 [07:12<00:00, 18.25it/s]\n",
      "Testing epoch  2: 100%|██████████| 7896/7896 [01:23<00:00, 94.83it/s] \n",
      "Training epoch 3:   0%|          | 3/7896 [00:00<05:46, 22.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: tensor(0.1163)\n",
      "train_pred: tensor([1.9398, 1.0694, 1.9587,  ..., 1.9484, 1.0891, 1.9549])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 3: 100%|██████████| 7896/7896 [07:00<00:00, 18.79it/s]\n",
      "Testing epoch  3: 100%|██████████| 7896/7896 [01:28<00:00, 88.93it/s] \n",
      "Training epoch 4:   0%|          | 2/7896 [00:00<07:18, 18.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: tensor(0.0931)\n",
      "train_pred: tensor([1.9634, 1.0237, 1.9721,  ..., 1.9667, 1.0269, 1.9711])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 4: 100%|██████████| 7896/7896 [07:29<00:00, 17.55it/s]\n",
      "Testing epoch  4: 100%|██████████| 7896/7896 [01:30<00:00, 87.55it/s] \n",
      "Training epoch 5:   0%|          | 3/7896 [00:00<06:39, 19.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: tensor(0.0772)\n",
      "train_pred: tensor([1.9757, 1.0180, 1.9809,  ..., 1.9777, 1.0216, 1.9818])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 5: 100%|██████████| 7896/7896 [07:17<00:00, 18.05it/s]\n",
      "Testing epoch  5: 100%|██████████| 7896/7896 [01:31<00:00, 86.20it/s] \n",
      "Training epoch 6:   0%|          | 2/7896 [00:00<06:45, 19.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: tensor(0.0652)\n",
      "train_pred: tensor([1.9818, 1.0104, 1.9859,  ..., 1.9851, 1.0102, 1.9870])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 6: 100%|██████████| 7896/7896 [07:39<00:00, 17.17it/s]\n",
      "Testing epoch  6: 100%|██████████| 7896/7896 [01:36<00:00, 82.11it/s] \n",
      "Training epoch 7:   0%|          | 3/7896 [00:00<05:45, 22.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: tensor(0.0574)\n",
      "train_pred: tensor([1.9876, 1.0074, 1.9890,  ..., 1.9883, 1.0074, 1.9900])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 7: 100%|██████████| 7896/7896 [07:18<00:00, 18.00it/s]\n",
      "Testing epoch  7: 100%|██████████| 7896/7896 [01:15<00:00, 104.73it/s]\n",
      "Training epoch 8:   0%|          | 2/7896 [00:00<06:48, 19.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: tensor(0.0497)\n",
      "train_pred: tensor([1.9899, 1.0061, 1.9901,  ..., 1.9905, 1.0059, 1.9921])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 8: 100%|██████████| 7896/7896 [06:03<00:00, 21.75it/s]\n",
      "Testing epoch  8: 100%|██████████| 7896/7896 [01:14<00:00, 106.18it/s]\n",
      "Training epoch 9:   0%|          | 3/7896 [00:00<06:44, 19.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: tensor(0.0434)\n",
      "train_pred: tensor([1.9890, 1.0039, 1.9906,  ..., 1.9911, 1.0034, 1.9939])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 9: 100%|██████████| 7896/7896 [06:17<00:00, 20.91it/s]\n",
      "Testing epoch  9: 100%|██████████| 7896/7896 [01:20<00:00, 98.36it/s] \n",
      "Training epoch 10:   0%|          | 0/7896 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: tensor(0.0390)\n",
      "train_pred: tensor([1.9921, 1.0038, 1.9923,  ..., 1.9934, 1.0031, 1.9949])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 10: 100%|██████████| 7896/7896 [06:00<00:00, 21.92it/s]\n",
      "Testing epoch  10: 100%|██████████| 7896/7896 [01:13<00:00, 107.18it/s]\n",
      "Training epoch 11:   0%|          | 3/7896 [00:00<04:46, 27.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: tensor(0.0348)\n",
      "train_pred: tensor([1.9930, 1.0028, 1.9938,  ..., 1.9950, 1.0024, 1.9959])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 11: 100%|██████████| 7896/7896 [05:52<00:00, 22.42it/s]\n",
      "Testing epoch  11: 100%|██████████| 7896/7896 [01:12<00:00, 109.63it/s]\n",
      "Training epoch 12:   0%|          | 3/7896 [00:00<04:52, 27.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: tensor(0.0317)\n",
      "train_pred: tensor([1.9929, 1.0022, 1.9947,  ..., 1.9956, 1.0019, 1.9965])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 12: 100%|██████████| 7896/7896 [05:56<00:00, 22.12it/s]\n",
      "Testing epoch  12: 100%|██████████| 7896/7896 [01:12<00:00, 109.52it/s]\n",
      "Training epoch 13:   0%|          | 3/7896 [00:00<05:26, 24.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: tensor(0.0292)\n",
      "train_pred: tensor([1.9954, 1.0019, 1.9964,  ..., 1.9967, 1.0017, 1.9970])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 13: 100%|██████████| 7896/7896 [05:52<00:00, 22.41it/s]\n",
      "Testing epoch  13: 100%|██████████| 7896/7896 [01:13<00:00, 107.48it/s]\n",
      "Training epoch 14:   0%|          | 3/7896 [00:00<05:05, 25.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: tensor(0.0268)\n",
      "train_pred: tensor([1.9955, 1.0018, 1.9967,  ..., 1.9970, 1.0015, 1.9973])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 14: 100%|██████████| 7896/7896 [05:57<00:00, 22.07it/s]\n",
      "Testing epoch  14: 100%|██████████| 7896/7896 [01:10<00:00, 111.73it/s]\n",
      "Training epoch 15:   0%|          | 3/7896 [00:00<05:12, 25.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: tensor(0.0287)\n",
      "train_pred: tensor([1.9968, 1.0020, 1.9971,  ..., 1.9975, 1.0014, 1.9977])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 15: 100%|██████████| 7896/7896 [05:44<00:00, 22.94it/s]\n",
      "Testing epoch  15: 100%|██████████| 7896/7896 [01:18<00:00, 100.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: tensor(0.0259)\n",
      "train_pred: tensor([1.9966, 1.0016, 1.9972,  ..., 1.9978, 1.0012, 1.9979])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    train_loss = trainer.train(train_set)\n",
    "    train_loss, train_pred = trainer.test(train_set)\n",
    "    print('train_loss:', train_loss)\n",
    "    print('train_pred:', train_pred)\n",
    "    checkpoint = {'model': trainer.model.state_dict(), 'optim': trainer.optimizer,\n",
    "                  'args': args, 'epoch': epoch}\n",
    "    torch.save(checkpoint, 'checkpoint_' + str(epoch) + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing epoch  0:   0%|          | 0/2265 [00:00<?, ?it/s]../learning/treelstm/trainer.py:50: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  linput, rinput = Var(lsent, volatile=True), Var(rsent, volatile=True)\n",
      "../learning/treelstm/trainer.py:51: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  target = Var(map_label_to_target(label, dataset.num_classes), volatile=True)\n",
      "/home/mateus/TCC/envSQG/lib/python3.5/site-packages/torch/nn/functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/mateus/TCC/envSQG/lib/python3.5/site-packages/torch/nn/functional.py:1339: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "../learning/treelstm/model.py:66: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = F.log_softmax(self.wp(out))\n",
      "/home/mateus/TCC/envSQG/lib/python3.5/site-packages/torch/nn/functional.py:1932: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "Testing epoch  0: 100%|██████████| 2265/2265 [00:22<00:00, 100.73it/s]\n",
      "Testing epoch  0:   0%|          | 5/2265 [00:00<00:46, 48.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 (0.8488633220603211, 0.848108381892653, 0.8484479832602387)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing epoch  0: 100%|██████████| 2265/2265 [00:22<00:00, 101.06it/s]\n",
      "Testing epoch  0:   0%|          | 8/2265 [00:00<00:28, 79.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 (0.8441505672148859, 0.8456562407873196, 0.8445672589748006)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing epoch  0: 100%|██████████| 2265/2265 [00:22<00:00, 102.12it/s]\n",
      "Testing epoch  0:   0%|          | 8/2265 [00:00<00:29, 77.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 (0.8390773901883333, 0.8409644382695528, 0.8386607628478607)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing epoch  0: 100%|██████████| 2265/2265 [00:21<00:00, 104.24it/s]\n",
      "Testing epoch  0:   0%|          | 11/2265 [00:00<00:21, 103.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 (0.8270811079252638, 0.82842696030158, 0.8251082568463877)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing epoch  0: 100%|██████████| 2265/2265 [00:21<00:00, 104.78it/s]\n",
      "Testing epoch  0:   0%|          | 6/2265 [00:00<00:37, 59.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 (0.8214998664473592, 0.8227546933706336, 0.8193752082609883)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing epoch  0: 100%|██████████| 2265/2265 [00:22<00:00, 102.47it/s]\n",
      "Testing epoch  0:   0%|          | 11/2265 [00:00<00:22, 101.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 (0.825507234371117, 0.8265714872637633, 0.8229269933178343)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing epoch  0: 100%|██████████| 2265/2265 [00:30<00:00, 74.88it/s] \n",
      "Testing epoch  0:   0%|          | 7/2265 [00:00<00:36, 61.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 (0.8133604429750928, 0.8140606672646415, 0.8101411655357724)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing epoch  0: 100%|██████████| 2265/2265 [00:31<00:00, 72.08it/s] \n",
      "Testing epoch  0:   0%|          | 10/2265 [00:00<00:24, 91.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 (0.8293490318322873, 0.8310249361776862, 0.8296202222493589)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing epoch  0: 100%|██████████| 2265/2265 [00:25<00:00, 89.45it/s] \n",
      "Testing epoch  0:   0%|          | 5/2265 [00:00<00:45, 49.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 (0.8041416283728055, 0.8035774210140063, 0.7986688644334486)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing epoch  0: 100%|██████████| 2265/2265 [00:26<00:00, 86.19it/s] \n",
      "Testing epoch  0:   0%|          | 6/2265 [00:00<00:38, 59.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 (0.8055789858434201, 0.8067716729286758, 0.8034766935846939)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing epoch  0: 100%|██████████| 2265/2265 [00:25<00:00, 88.27it/s] \n",
      "Testing epoch  0:   0%|          | 7/2265 [00:00<00:33, 68.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 (0.8138371699522639, 0.815571938881118, 0.8138073479675143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing epoch  0: 100%|██████████| 2265/2265 [00:25<00:00, 87.97it/s] \n",
      "Testing epoch  0:   0%|          | 5/2265 [00:00<00:50, 44.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 (0.8203684309921542, 0.8218800767749503, 0.818892726923815)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing epoch  0: 100%|██████████| 2265/2265 [00:25<00:00, 88.72it/s] \n",
      "Testing epoch  0:   0%|          | 9/2265 [00:00<00:25, 88.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 (0.8193450753729594, 0.8205679558669485, 0.817169754814719)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing epoch  0: 100%|██████████| 2265/2265 [00:27<00:00, 83.55it/s] \n",
      "Testing epoch  0:   0%|          | 9/2265 [00:00<00:26, 86.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 (0.7883769119029309, 0.7857785224585547, 0.7795747691720813)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing epoch  0: 100%|██████████| 2265/2265 [00:26<00:00, 85.88it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 (0.8013806161492643, 0.7995218814880791, 0.7937609346158474)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    checkpoint = torch.load('checkpoint_' + str(epoch) + '.pth')\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    loss, pred = trainer.test(dev_set)\n",
    "    print('epoch', epoch, metrics.f1(pred.numpy(), dev_set.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('checkpoint_14.pth')\n",
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 3.1492e-06, -7.0746e-40,  6.9912e-40,  ..., -3.4392e-03,\n",
       "         -4.6204e-04, -2.1826e-03],\n",
       "        ...,\n",
       "        [-7.7178e-02, -5.1056e-02, -8.5519e-02,  ..., -3.8988e-02,\n",
       "         -4.5611e-03,  6.5343e-03],\n",
       "        [-2.9133e-04,  3.0484e-04,  6.9033e-04,  ...,  3.2669e-04,\n",
       "          4.2872e-05,  5.6230e-05],\n",
       "        [ 8.6759e-02,  1.3279e-02, -2.2038e-01,  ...,  1.2827e-02,\n",
       "         -1.1806e-01, -1.4734e-01]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.emb.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8013806161492643, 0.7995218814880791, 0.7937609346158474)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1(pred.numpy(), dev_set.labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
