{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import random\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from learning.treelstm.utils import load_word_vectors\n",
    "from learning.treelstm.trainer import Trainer\n",
    "from learning.treelstm.metrics import Metrics\n",
    "from learning.treelstm.model import *\n",
    "from learning.treelstm.vocab import Vocab\n",
    "import learning.treelstm.Constants as Constants\n",
    "from learning.treelstm.dataset import QGDataset\n",
    "from learning.treelstm.scripts.preprocess_lcquad import build_vocab\n",
    "from itertools import product\n",
    "\n",
    "data_path = '../learning/treelstm/data/lc_quad/'\n",
    "train_path = data_path + 'train/'\n",
    "dev_path = data_path + 'dev/'\n",
    "test_path = data_path + 'test/'\n",
    "checkpoints_path = '../learning/treelstm/checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.get_num_threads()\n",
    "torch.set_num_threads(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Struct: pass\n",
    "args = Struct()\n",
    "args.seed = 41\n",
    "args.cuda = False\n",
    "args.batchsize = 20\n",
    "args.mem_dim = 150\n",
    "args.hidden_dim = 50\n",
    "args.num_classes = 2\n",
    "args.input_dim = 300\n",
    "args.sparse = False\n",
    "args.lr = 0.01\n",
    "args.wd = 1e-4\n",
    "\n",
    "args.epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(\n",
    "    os.path.join(data_path, 'dataset.vocab'),\n",
    "    [Constants.PAD_WORD, Constants.UNK_WORD, Constants.BOS_WORD, Constants.EOS_WORD]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    emb = torch.load('glove_lc_merged_emb.pth')\n",
    "except:\n",
    "    emb = torch.Tensor(vocab.size(), 300).normal_(-0.05, 0.05)\n",
    "    # zero out the embeddings for padding and other special words if they are absent in vocab\n",
    "    for idx, item in enumerate([Constants.PAD_WORD, Constants.UNK_WORD, Constants.BOS_WORD, Constants.EOS_WORD]):\n",
    "        emb[idx].zero_()\n",
    "\n",
    "    with open('../learning/treelstm/data/glove.840B.300d.txt', 'r') as file:\n",
    "        for line in tqdm(file):\n",
    "            contents = line.rstrip('\\n').split(' ')\n",
    "            idx = vocab.getIndex(contents[0])\n",
    "            if(idx):\n",
    "                emb[idx] = torch.Tensor(list(map(float, contents[1:])))\n",
    "\n",
    "    torch.save(emb, 'glove_lc_merged_emb.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8da472736954b06b94526b1a82e26d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7896), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8860032c53fa4679b27f654d3fa9d357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7896), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2619d0b2ea44e1b34475373ef354f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7896), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcea30f485eb4014976631f7eacc37ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2265), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bcc1f4d226643b2a201591b1346d0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2265), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3115c0fa70664c039738a19b000d7f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2265), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20cb040e4cfc4c3381cead606753e0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1090), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4997e950a94637a043a003eea615fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1090), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b130d592e7e74e59908a59dc3db35f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1090), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_set = QGDataset(train_path, vocab, BertTokenizer.from_pretrained('bert-base-uncased'), args.num_classes)\n",
    "dev_set = QGDataset(dev_path, vocab, BertTokenizer.from_pretrained('bert-base-uncased'), args.num_classes)\n",
    "test_set = QGDataset(test_path, vocab, BertTokenizer.from_pretrained('bert-base-uncased'), args.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = DASimilarity(args.mem_dim, args.hidden_dim, args.num_classes)\n",
    "#similarity = CosSimilarity(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimilarityEncodersBERT(vocab.size(), args.input_dim, args.mem_dim, similarity, args.sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.KLDivLoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=args.lr, weight_decay=args.wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.4421, -0.1307,  0.1318,  ..., -0.6867, -0.5819, -0.6584],\n",
       "        ...,\n",
       "        [-0.0736, -0.0562, -0.0973,  ..., -0.0250, -0.0307,  0.0238],\n",
       "        [ 0.1746,  0.1117,  0.6059,  ..., -0.3987, -0.4725,  0.1404],\n",
       "        [ 0.2707, -0.0874, -0.3683,  ...,  0.0368, -0.2172, -0.3406]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.emb.weight.data.copy_(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TrainerBERT(args, model, criterion, optimizer)\n",
    "metrics = Metrics(args.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69116719d2a47e59d804c5ef6abd546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training epoch 1', max=7896, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mateus/TCC/envSQG/lib/python3.5/site-packages/torch/nn/functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/mateus/TCC/envSQG/lib/python3.5/site-packages/torch/nn/functional.py:1339: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "../learning/treelstm/model.py:66: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = F.log_softmax(self.wp(out))\n",
      "/home/mateus/TCC/envSQG/lib/python3.5/site-packages/torch/nn/functional.py:1932: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 93763584 bytes. Error code 12 (Cannot allocate memory)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-2b9eee2e0144>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_pred:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-a84550e5eecd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mk\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchsize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TCC/envSQG/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TCC/envSQG/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 93763584 bytes. Error code 12 (Cannot allocate memory)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    train_loss = trainer.train(train_set)\n",
    "    train_loss, train_pred = trainer.test(train_set)\n",
    "    print('train_loss:', train_loss)\n",
    "    print('train_pred:', train_pred)\n",
    "    checkpoint = {'model': trainer.model.state_dict(), 'optim': trainer.optimizer,\n",
    "                  'args': args, 'epoch': epoch}\n",
    "    torch.save(checkpoint, 'checkpoint_bert_' + str(epoch) + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left - a - sent\n",
    "# right - b - query\n",
    "\n",
    "class SimilarityEncodersBERT(nn.Module):\n",
    "    def __init__(self, vocab_size, in_dim, mem_dim, similarity, sparsity):\n",
    "        super(SimilarityEncodersBERT, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, in_dim, padding_idx=Constants.PAD, sparse=sparsity)\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.query_treelstm = ChildSumTreeLSTM(in_dim, mem_dim)\n",
    "        self.bert_converter = nn.Linear(768, mem_dim)\n",
    "        self.similarity = similarity\n",
    "\n",
    "    # receber sequencia de indexes para a sentenca\n",
    "    def forward(self, linputs, rtree, rinputs):\n",
    "        rinputs = self.emb(rinputs)\n",
    "        bert_raw = self.bert(linputs.unsqueeze(0))[0][[0],0,:]\n",
    "        # above: gets the hidden state of the last layer of the stack\n",
    "        # and get the representation of the [CLS] token.\n",
    "        bert_reduced = self.bert_converter(bert_raw)\n",
    "        rstate, rhidden = self.query_treelstm(rtree, rinputs)\n",
    "        output = self.similarity(bert_reduced, rstate)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable as Var\n",
    "from learning.treelstm.utils import map_label_to_target\n",
    "\n",
    "class TrainerBERT(object):\n",
    "    def __init__(self, args, model, criterion, optimizer):\n",
    "        super(TrainerBERT, self).__init__()\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.epoch = 0\n",
    "\n",
    "    # helper function for training\n",
    "    def train(self, dataset):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss, k = 0.0, 0\n",
    "        indices = torch.randperm(len(dataset))\n",
    "        for idx in tqdm(range(len(dataset)), desc='Training epoch ' + str(self.epoch + 1) + ''):\n",
    "            lsent, rtree, rsent, label = dataset[indices[idx]]\n",
    "            linput, rinput = Var(lsent), Var(rsent)\n",
    "            target = Var(map_label_to_target(label, dataset.num_classes))\n",
    "            if self.args.cuda:\n",
    "                linput, rinput = linput.cuda(), rinput.cuda()\n",
    "                target = target.cuda()  \n",
    "            output = self.model(linput, rtree, rinput)\n",
    "            err = self.criterion(output, target)\n",
    "            loss += err.data.item()\n",
    "            err.backward()\n",
    "            k += 1\n",
    "            if k % self.args.batchsize == 0:\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "        self.epoch += 1\n",
    "        return loss / len(dataset)\n",
    "\n",
    "    # helper function for testing\n",
    "    def test(self, dataset):\n",
    "        self.model.eval()\n",
    "        loss = 0\n",
    "        predictions = torch.zeros(len(dataset))\n",
    "        indices = torch.arange(1, dataset.num_classes + 1, dtype=torch.float)\n",
    "        for idx in tqdm(range(len(dataset)), desc='Testing epoch  ' + str(self.epoch) + ''):\n",
    "            lsent, rtree, rsent, label = dataset[idx]\n",
    "            linput, rinput = Var(lsent, volatile=True), Var(rsent, volatile=True)\n",
    "            target = Var(map_label_to_target(label, dataset.num_classes), volatile=True)\n",
    "            if self.args.cuda:\n",
    "                linput, rinput = linput.cuda(), rinput.cuda()\n",
    "                target = target.cuda()\n",
    "            output = self.model(linput, rtree, rinput)\n",
    "            err = self.criterion(output, target)\n",
    "            loss += err.data\n",
    "            output = output.data.squeeze().cpu()\n",
    "            predictions[idx] = torch.dot(indices, torch.exp(output))\n",
    "        return loss / len(dataset), predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from copy import deepcopy\n",
    "import torch.utils.data as data\n",
    "from transformers import BertTokenizer\n",
    "from learning.treelstm.tree import Tree\n",
    "\n",
    "class QGDataset(data.Dataset):\n",
    "    def __init__(self, path, vocab, bert_tok, num_classes):\n",
    "        super(QGDataset, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.bert_tok = bert_tok\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Converte os tokens para indices do vocabularios\n",
    "        self.lsentences = self.read_sentences(os.path.join(path, 'a.txt'), bert=True)\n",
    "        self.rsentences = self.read_sentences(os.path.join(path, 'b.toks'))\n",
    "        self.rtrees = self.read_trees(os.path.join(path, 'b.parents'))\n",
    "\n",
    "        # cria tensor de labels\n",
    "        self.labels = self.read_labels(os.path.join(path, 'sim.txt'))\n",
    "\n",
    "        self.size = len(self.lsentences)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        lsent = deepcopy(self.lsentences[index])\n",
    "        rtree = deepcopy(self.rtrees[index])\n",
    "        rsent = deepcopy(self.rsentences[index])\n",
    "        label = deepcopy(self.labels[index])\n",
    "        return (lsent, rtree, rsent, label)\n",
    "\n",
    "    def read_sentences(self, filename, bert=False):\n",
    "        with open(filename, 'r') as f:\n",
    "            sentences = [self.read_sentence(line, bert) for line in tqdm(f.readlines())]\n",
    "        return sentences\n",
    "\n",
    "    def read_sentence(self, line, bert):\n",
    "        if(bert):\n",
    "            indices = self.bert_tok.encode(line, add_special_tokens=True)\n",
    "        else:\n",
    "            indices = self.vocab.convertToIdx(line.split(), Constants.UNK_WORD)\n",
    "        return torch.LongTensor(indices)\n",
    "\n",
    "    def read_trees(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            trees = [self.read_tree(line) for line in tqdm(f.readlines())]\n",
    "        return trees\n",
    "\n",
    "    def read_tree(self, line):\n",
    "        parents = list(map(int, line.split()))\n",
    "        trees = dict()\n",
    "        root = None\n",
    "        for i in range(1, len(parents) + 1):\n",
    "            if i - 1 not in trees.keys() and parents[i - 1] != -1:\n",
    "                idx = i\n",
    "                prev = None\n",
    "                while True:\n",
    "                    parent = parents[idx - 1]\n",
    "                    if parent == -1:\n",
    "                        break\n",
    "                    tree = Tree()\n",
    "                    if prev is not None:\n",
    "                        tree.add_child(prev)\n",
    "                    trees[idx - 1] = tree\n",
    "                    tree.idx = idx - 1\n",
    "                    if parent - 1 in trees.keys():\n",
    "                        trees[parent - 1].add_child(tree)\n",
    "                        break\n",
    "                    elif parent == 0:\n",
    "                        root = tree\n",
    "                        break\n",
    "                    else:\n",
    "                        prev = tree\n",
    "                        idx = parent\n",
    "        return root\n",
    "\n",
    "    def read_labels(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            labels = list(map(lambda x: float(x), f.readlines()))\n",
    "            labels = torch.Tensor(labels)\n",
    "        return labels\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
